{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Add WIAG roles, institutions, institution roles and offices to FactGrid\n",
    "\n",
    "\n",
    "\n",
    "This notebook takes data from WIAG as the primary source and joins it with data from FactGrid to find out which institutions, roles or institution roles need to be added to FactGrid. After generating files for adding these things to FactGrid (institutions are now instead added using the workflow developed for [DomVoc](https://github.com/Germania-Sacra/DomVoc/tree/main)), at the end a file for adding all offices of persons to FactGrid is generated.\n",
    "\n",
    "\n",
    "\n",
    "Whenever a file is generated, all entries with institutions/roles/inst roles, that still need to be added to FG, are from then on ignored so the other steps can be performed for the rest of the entries. This enables you to go through the notebook linearly from start to finish and add all four things for the majority of entries right away. This means though, that you will need to go through the notebook multiple (up to four) times to make sure FactGrid is all up-to-date.\n",
    "\n",
    "\n",
    "\n",
    "Another possible workflow for the notebook is to use the generated files right away whenever one is generated to add things to FG and after that is done, start from the beginning of the notebook again. This way you also need to go through the notebook multiple times, but only need to upload one file per thing (institutions, roles, inst roles, offices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, timezone, date\n",
    "import math\n",
    "import traceback\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "from enum import Enum\n",
    "\n",
    "today_string = datetime.now().strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the automatic translation, AI models hosted by the GWDG are used. For this a [SAIA](https://docs.hpc.gwdg.de/services/saia/index.html) API key is needed. You can either uncomment the line in the cell below and replace the placeholder with your key or (safer option) create a text-file (called `.env`) in the project directory containing `API_KEY=\"PLACEHOLDER\"` (with your key inserted) before running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API_KEY=\"PLACEHOLDER\"\n",
    "\n",
    "import scripts.translate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below defines where input files can be found and where the generated files will be saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = r\"C:\\Users\\Public\\sync_notebooks\\input_files\"\n",
    "\n",
    "output_path = r\"C:\\Users\\Public\\sync_notebooks\\output_files\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download data from WIAG\n",
    "\n",
    "\n",
    "\n",
    "### Export data via phpMyAdmin\n",
    "\n",
    "\n",
    "\n",
    "For this step you have to manually export the datasets by following the steps. In case the text description is not enough, a description with screenshots can be found [here](https://github.com/WIAG-ADW-GOE/sync_notebooks/blob/main/docs/Run_SQL_Query_and_Export_CSV.md).\n",
    "\n",
    "\n",
    "\n",
    "1. open [phpMyAdmin WIAG](https://vwebfile.gwdg.de/phpmyadmin/)\n",
    "\n",
    "2. log in\n",
    "\n",
    "3. select the 'wiagvokabulare' database\n",
    "\n",
    "4. switch to the 'SQL' tab\n",
    "\n",
    "5. copy [this query](queries/get_wiag_roles.sql), paste it in the text field and click 'Go'\n",
    "\n",
    "6. export the result to a csv file\n",
    "\n",
    "### Import the files\n",
    "\n",
    "\n",
    "\n",
    "Please move the downloaded file to the `input_path` directory defined above or change the `input_path` to where the file is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = f'role.csv'\n",
    "input_path_file = os.path.join(input_path, input_file)\n",
    "wiag_roles_df = pl.read_csv(input_path_file)\n",
    "len(wiag_roles_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export data via the website\n",
    "\n",
    "\n",
    "\n",
    "It's recommended to limit the export to one Domstift by first searching for that Domstift before exporting the 'CSV Amtsdaten' to make sure that the amount of roles to be added is manageable.\n",
    "\n",
    "\n",
    "\n",
    "1. go to https://wiag-vocab.adw-goe.de/query/can\n",
    "\n",
    "2. filter by cathedral chapter (Domstift)\n",
    "\n",
    "3. click Export->Amtsdaten\n",
    "\n",
    "\n",
    "\n",
    "If you filtered by Domstift (cathedral chapter), **change the variable below** to the domstift you used and **change the name of the exported file** to include the name of the cathedral chapter.\n",
    "\n",
    "\n",
    "\n",
    "If you did not filter, you need to change the line to `domstift = \"\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domstift = \"Bamberg\" # with domstift = \"Mainz\" the name of the file should be \"WIAG-Domherren-DB-Ämter-Mainz.csv\"\n",
    "#domstift = \"\" # in case you did not filter by Domstift, use this instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if domstift == \"\":\n",
    "    input_file = f'WIAG-Domherren-DB-Ämter.csv'\n",
    "else:\n",
    "    input_file = f'WIAG-Domherren-DB-Ämter-' + domstift + '.csv'\n",
    "\n",
    "input_path_file = os.path.join(input_path, input_file)\n",
    "wiag_offices_df = pl.read_csv(input_path_file, separator=';', infer_schema_length = None)\n",
    "len(wiag_offices_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_modified = datetime.fromtimestamp(os.path.getmtime(input_path_file))\n",
    "now = datetime.now()\n",
    "assert last_modified.day == now.day and last_modified.month == now.month, f'The file was last updated on {last_modified.strftime('%d.%m')}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Troubleshooting: Old file used\n",
    "\n",
    "\n",
    "\n",
    "You get an error when you run the line above if the file was not updated today.\n",
    "\n",
    "Suggested solutions:\n",
    "\n",
    "* update the file again by downloading it again\n",
    "\n",
    "* if you downloaded the data today, check the file name in input_file. It's pointing to a file that has old data.\n",
    "\n",
    "* (not recommended) continue if you are sure that you need to use old data. This is something that the developer might want to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download data from FactGrid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Troubleshooting: If the following cell throws an error, try rerunning the cell. Its probably just a connection problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.wiag_to_factgrid_functions import load_fg_data\n",
    "\n",
    "(factgrid_institution_df, factgrid_diocese_df, factgrid_inst_roles_df) = load_fg_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for possible institution duplicates\n",
    "\n",
    "\n",
    "\n",
    "If any two (or more) institutions on FactGrid link to the same GSN, they will be listed below. These entries need to be **fixed manually**. Use the `fg_institution_id` to find and fix the entries on FG.\n",
    "\n",
    "\n",
    "\n",
    "You can either fix the duplicates now or just continue on and fix them later, because they will be ignored for the rest of the notebook.\n",
    "\n",
    "Not fixing them now means though, that you will need to run the notebook again at a later point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_fg_entries = factgrid_institution_df.group_by('fg_gsn_id').len().filter(pl.col('len') > 1)\n",
    "if not duplicate_fg_entries.is_empty():\n",
    "    display(factgrid_institution_df.filter(pl.col('fg_gsn_id').is_in(duplicate_fg_entries.get_column('fg_gsn_id').implode())))\n",
    "\n",
    "factgrid_institution_df = factgrid_institution_df.filter(pl.col('fg_gsn_id').is_in(duplicate_fg_entries.get_column('fg_gsn_id').implode()).not_())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Join the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the WIAG \"Amtsdaten\" for Domherren export is joined with institution data from FactGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiag_offices_df = wiag_offices_df.join(factgrid_institution_df, how='left', left_on='institution_id', right_on='fg_gsn_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the diocese data is added.\n",
    "\n",
    "\n",
    "\n",
    "For each entry in the input dataframe, the associated diocese is searched in the factgrid_diocese_df dataframe. The diocese is found by first searching for the WIAG-ID. Only if no entry was found, the search continues with the diocese's name, first in the diocese label and lastly, if the search was unsuccessfull again, in the diocese alt label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join with fg dioceses\n",
    "rows = []\n",
    "query = pl.DataFrame() # empty initialisation to enable the call of the clear function below\n",
    "\n",
    "for row in wiag_offices_df.iter_rows(named = True):\n",
    "    query = query.clear()\n",
    "\n",
    "    if row['diocese_id'] != None:\n",
    "        query = factgrid_diocese_df.filter(pl.col('dioc_wiag_id') == row['diocese_id'])\n",
    "        \n",
    "    if query.is_empty() and row['diocese'] != None:\n",
    "        query = factgrid_diocese_df.filter(pl.col('dioc_label') == row['diocese'])\n",
    "        \n",
    "        if query.is_empty():\n",
    "            query = factgrid_diocese_df.filter(pl.col('dioc_alt') == row['diocese'])\n",
    "\n",
    "    if not query.is_empty():\n",
    "        rows.append({'role_all-id': row['id'], 'fg_diocese_id': query.row(0)[0]})\n",
    "    # #TODO should cases where no result was found be noted/handled?\n",
    "\n",
    "wiag_offices_df = wiag_offices_df.join(pl.DataFrame(rows), how = 'left', left_on = 'id', right_on = 'role_all-id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Missing institutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for special cases\n",
    "\n",
    "\n",
    "\n",
    "These lists below allow the code below to identify if the role is missing an institution or if the role doesn't require one at all.\n",
    "\n",
    "* The `unbound_role_groups` list contains the role_groups that are not bound to a place at all.\n",
    "\n",
    "* The `diocese_role_groups` list contains the role_groups that are bound to a diocese but not an institution.\n",
    "\n",
    "  * `diocese_role_group_exception_roles` contains roles that belong to this group but are still bound to an institution.\n",
    "\n",
    "Please add more role_groups or roles to the lists if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbound_role_groups = [\n",
    "    'Kurienamt',\n",
    "    'Papst',\n",
    "    'Kardinal',\n",
    "]\n",
    "diocese_role_groups = [\n",
    "    'Oberstes Leitungsamt Diözese',\n",
    "    'Leitungsamt Diözese',\n",
    "    'Bischöfliches Hilfspersonal',\n",
    "]\n",
    "diocese_role_group_exception_roles = [ \n",
    "    'Erzbischöflicher Prokurator',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select all entries that should contain an institution on FactGrid but don't have it after the join operation\n",
    "missing_inst_df = wiag_offices_df.filter(\n",
    "    pl.col('fg_institution_id').is_null() &\n",
    "    pl.col('role_group').is_in(unbound_role_groups).not_() &\n",
    "    pl.col('role_group').is_in(diocese_role_groups).not_()\n",
    ")\n",
    "print(str(missing_inst_df.height) + \" entries with missing institution id in FG\")\n",
    "\n",
    "#select all entries that should contain a diocese on FactGrid but don't have it after the join operation\n",
    "missing_dioc_df = wiag_offices_df.filter(\n",
    "    pl.col('fg_diocese_id').is_null() & \n",
    "    pl.col('role_group').is_in(unbound_role_groups).not_() & \n",
    "    pl.col('role_group').is_in(diocese_role_groups) &\n",
    "    pl.col('name').is_in(diocese_role_group_exception_roles).not_()\n",
    ")\n",
    "print(str(missing_dioc_df.height) + \" entries with missing diocese id in FG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for new roles (roles that so far have not been handled by this notebook)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Any roles showing up here need to be added to the `diocese_role_group_exception_roles` list if they don't need a diocese entry in FactGrid. If you added a name to the `diocese_role_group_exception_roles` list, rerun the cells from the start of step 5 to make sure the change is propagated.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - diocese is missing WIAG\n",
    "\n",
    " - diocese is missing in FG\n",
    "\n",
    " - diocese in FG does not have the Q164535 property\n",
    "\n",
    " - diocese in FG has a different German label and no WIAG ID -> list label from WIAG as an alternative label\n",
    "\n",
    " - diocese in FG is missing German label and WIAG ID -> add German label\n",
    "\n",
    " - role that should not be uploaded to FactGrid\n",
    "\n",
    " - incorrect office assignment in WIAG\n",
    "\n",
    " - a role that should be added to unbound_roles\n",
    "\n",
    " - role with incorrect role group assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roles_that_need_a_diocese = ['Bischof','Koadjutor','Erzbischof']\n",
    "roles_that_need_a_diocese = ['Archipresbyter','Propst und Archidiakon']\n",
    "missing_dioc_df.filter(pl.col('name').is_in(roles_that_need_a_diocese).not_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check entries that have no role group in wiag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_inst_df.filter(pl.col('role_group').is_null())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for entries that are missing an id **in WIAG** required for the join\n",
    "\n",
    "Please **manually inspect all the entries** that are shown by the code cells below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entries that have a missing institution id **in WIAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_inst_df.filter(pl.col('institution_id').is_null())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entries that have a missing diocese id **in WIAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dioc_df.filter(pl.col('diocese_id').is_null())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing institutions\n",
    "\n",
    "\n",
    "\n",
    "If there are any institutions listed here, they should be added using the workflow that was designed for adding monasteries as part of the DomVoc project. The code can be found on [GitHub](https://github.com/Germania-Sacra/DomVoc/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_institution_factgrid_df = missing_inst_df.filter(pl.col('institution_id').is_not_null()).rename({'institution' : 'Lde', 'institution_id' : 'P471'}).unique(subset = pl.col('P471')).with_columns(\n",
    "    qid = None,\n",
    "    Len = None,\n",
    "    Dde = None,\n",
    "    Den = None,\n",
    "    P131 = pl.lit('Q153178')\n",
    ").select(['qid', 'Lde', 'Len',\t'P471',\t'Dde',\t'Den',\t'P131'])\n",
    "\n",
    "create_institution_factgrid_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Missing roles\n",
    "\n",
    "\n",
    "\n",
    "These roles do not include the institution information. In other words, this step adds roles to FactGrid like 'archbishop' and not 'archbishop of trier'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all missing (institution and diocese) entries **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_missing_entries = pl.concat([missing_inst_df, missing_dioc_df], how = \"diagonal\")\n",
    "\n",
    "dioc_joined_df = wiag_offices_df.remove(pl.col(\"id\").is_in(all_missing_entries.get_column(\"id\").implode()))\n",
    "\n",
    "print(\"From originally \" + str(wiag_offices_df.height) + \" rows, \" + str(dioc_joined_df.height) + \" rows, that are not missing an institution or diocese, are left.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for special cases\n",
    "\n",
    "\n",
    "\n",
    "#### Check for roles with multiple entries in FactGrid\n",
    "\n",
    "\n",
    "\n",
    "Should the cell below print anything, these entries need to be **handled manually**, because they contain more than one entry on FactGrid. You can continue with the rest of the notebook even without taking care of these, because these entries will simply be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiag_roles_df.filter(pl.col(\"name\").is_duplicated())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for missing roles in WIAG role table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_roles_wiag = dioc_joined_df.filter(pl.col(\"name\").is_in(wiag_roles_df.get_column(\"name\").implode()).not_()).unique()\n",
    "print(missing_roles_wiag.height)\n",
    "missing_roles_wiag.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join role_fg_id attribute from WIAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiag_roles_df = wiag_roles_df.remove(pl.col(\"name\").is_duplicated())\n",
    "\n",
    "joined_df = dioc_joined_df.join(wiag_roles_df.rename({'id' : 'role_id', 'factgrid_id': 'role_fg_id'}), on = \"name\", how = \"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ignore all Kanonikatsbewerber and Vikariatsbewerber roles/offices\n",
    "\n",
    "\n",
    "\n",
    "The 'bewerber' suffix means, that this person was applying for this office, so these are not proper offices and don't need to be / shouldn't be added to FactGrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = joined_df.remove(pl.col('name').is_in(['Vikariatsbewerber', 'Kanonikatsbewerber']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entries with missing FactGrid-entries for the roles in wiag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_roles_df = joined_df.filter(pl.col('role_fg_id').is_null())\n",
    "print(str(missing_roles_df.height) + \" entries are missing a role in FactGrid.\\n\")\n",
    "\n",
    "print(\"Roles that are not yet in FactGrid:\")\n",
    "missing_roles = missing_roles_df.select(pl.col('name'), pl.col('role_id'), pl.col('role_group_fq_id')).unique().drop_nulls() # TODO report null values, instead of just dropping them\n",
    "missing_roles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate missing roles file\n",
    "\n",
    "\n",
    "\n",
    "In this step a file is prepared for the roles missing in FactGrid that can later be uploaded to add the roles to FG. Important things to note:\n",
    "\n",
    "\n",
    "\n",
    "1. The English labels for the roles are translations of the German labels. To facilitate the process of translating, a first draft of translations is generated using AI. However, the quality varies widly and it is **absolutely necessary** to check and correct the translations, since some will likely be incorrect.\n",
    "\n",
    "2. The generated file also contains columns for descriptions. Either fill in the descriptions or if you do not intend to add any, you should remove these columns (with e.g. Excel or LibreOffice Calc). Should you want to add descriptions only for a few roles, it might still be easier to remove the columns and add the descriptions separately after the upload, depending on the number of roles to be uploaded (FactGrid does not allow empty cells for the csv format, so you need to add descriptions for every row or none at all)\n",
    "\n",
    "\n",
    "\n",
    "After checking/correcting the translations and adding descriptions or removing the columns, you can copy the content of the generated file (name: `create-missing-roles_<date>.csv`) and paste it into the textfield on quickstatements. As mentioned above, you can either do this right away or at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell generates the translations of the labels. This can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"**Role:** You are a professional translator specializing in historical and religious terminology, with expertise in German–English translation.\n",
    "    **Task:** You will receive a German name for a role or occupation. Your task is to return the most accurate and context-appropriate English translation.\n",
    "    **Format:** Only return the translation. Do not add any remarks or formatting. Always start the translation with a capital letter.\"\"\"\n",
    "    \n",
    "create_missing_roles_df = scripts.translate.translate(missing_roles.rename({\"name\" : \"Lde\"}), system_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this cell generates the file and show a sample of the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_missing_roles_df = create_missing_roles_df.with_columns(\n",
    "    qid = None,\n",
    "    Dde = None,\n",
    "    Den = None,\n",
    "    P2 = pl.lit(\"Q37073\"),\n",
    "    P131 = pl.lit(\"Q153178\")\n",
    ").rename({\n",
    "    \"role_id\" : \"item_id\",\n",
    "    \"role_group_fq_id\" : \"P3\"}\n",
    ").select(\n",
    "    [\"qid\",\t\"Lde\",\t\"Len\",\t\"Dde\",\t\"Den\",\t\"P2\",\t\"P131\",\t\"item_id\",\t\"P3\"]\n",
    ")\n",
    "\n",
    "create_missing_roles_df.write_csv(os.path.join(output_path, f\"create-missing-roles_{today_string}.csv\"))\n",
    "print(f'{create_missing_roles_df.height} rows were written. Here is a sample of them:')\n",
    "if create_missing_roles_df.height >= 3:\n",
    "    display(create_missing_roles_df.sample(n=3))\n",
    "else:\n",
    "    display(create_missing_roles_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Missing institution roles\n",
    "\n",
    "\n",
    "\n",
    "### Remove all missing (role) entries now **\n",
    "\n",
    "\n",
    "\n",
    "The code below removes all the entries that failed the join with the WIAG role join above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_roles_in_fg_df = joined_df.remove(pl.col('role_fg_id').is_null())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for people with missing FactGrid-entries or missing FactGrid-IDs in wiag\n",
    "\n",
    "\n",
    "\n",
    "There generally shouldn't be any such persons, since notebook 3 takes care of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_people_list = joined_df.filter(pl.col('FactGrid').is_null()).unique('person_id')\n",
    "print(missing_people_list.height)\n",
    "missing_people_list.sample(n = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate quickstatements for creating the persons, go back to [notebook 3](Csv2FactGrid-create.ipynb) (Csv2FactGrid-create).\n",
    "\n",
    "\n",
    "\n",
    "The code below removes all the entries for persons that don't exist on FactGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(joined_df))\n",
    "joined_df = joined_df.filter(pl.col('FactGrid').is_not_null())\n",
    "print(len(joined_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out which institution roles are missing on FactGrid\n",
    "\n",
    "\n",
    "\n",
    "these roles have information of the institution as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in addition to the parameters, this uses the dataframe factgrid_inst_roles_df directly\n",
    "\n",
    "def find_fg_inst_role(name, inst, dioc):\n",
    "    search_result = pl.DataFrame()\n",
    "    if inst == None:\n",
    "        if dioc != None: # TODO handle cases where inst and dioc are None? - should only be true for [35, 48, 49] Kardinal, Papst, Kurienamt (except maybe special role_groups)\n",
    "            if name not in [\"Archidiakon\", \"Koadjutor\"]:\n",
    "                dioc = dioc.lstrip('Bistum').lstrip('Erzbistum').lstrip('Patriarchat').lstrip()\n",
    "            if name == \"Fürstbischof\" and dioc in [\"Passau\", \"Straßburg\"]:\n",
    "                name = \"Bischof\"    \n",
    "            search_result = factgrid_inst_roles_df.filter(pl.col('inst_role').str.contains(f\"^{name}.*{dioc}\"))\n",
    "            if name == \"Erzbischof\" and dioc == \"Salzburg\":\n",
    "                # will be merged in later # TODO what does this mean and why?\n",
    "                search_result = factgrid_inst_roles_df.filter(pl.col('fg_inst_role_id') == 'Q172567')\n",
    "    else:\n",
    "        name = name.replace('Domkanoniker', 'Domherr')\n",
    "        search_result = factgrid_inst_roles_df.filter(pl.col('inst_role') == f\"{name} {inst}\")\n",
    "    \n",
    "    return search_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = [] # joined to the main df as fg_inst_role_id (used in the last part) - in other words, these are the institution roles that are assigned on FactGrid\n",
    "not_found = [] # used for creating institution roles (e.g. bishop of ...) in the next cell\n",
    "dupl = {} # these entries are ignored, because they need to be fixed manually\n",
    "\n",
    "i = 0\n",
    "for (id, name, inst, inst_id, dioc) in joined_df.select('id', 'name', 'institution', 'institution_id', 'diocese').iter_rows():\n",
    "    # Kardinal receives insitution role Q254893 manually -- probably simply handling a simple special case first\n",
    "    if name == \"Kardinal\":\n",
    "        data_dict.append((id,\"Q254893\"))\n",
    "        continue\n",
    "    \n",
    "    search_result = find_fg_inst_role(name, inst, dioc)\n",
    "\n",
    "    if search_result.is_empty() or len(search_result) == 0:\n",
    "        # TODO entries without institution entry in WIAG are simply ignored - makes sense if dioc is set?? (diocese level roles)\n",
    "        not_found.append((name, inst, inst_id))\n",
    "    elif len(search_result) == 1:\n",
    "        data_dict.append((id, search_result['fg_inst_role_id'][0]))\n",
    "    elif len(search_result) >= 2:\n",
    "        dupl[i] = (name, inst, dioc, search_result)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "print(\"Roles found:\", len(data_dict), \"duplicates:\", len(dupl), \"not found:\", len(not_found))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate missing institution roles file\n",
    "\n",
    "\n",
    "\n",
    "This step is mostly the same (some additional preprocessing steps) as for the roles (without institution). It is no less important though, to check and **correct the translations** of the labels.\n",
    "\n",
    "\n",
    "\n",
    "Once again you also need to either add descriptions (for all the rows) or remove the description columns. Afterwards you can copy the content of the generated file (name: `create-missing-inst-roles_<date>.csv`) and paste it into the textfield on quickstatements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found_df = pl.DataFrame(not_found, orient = 'row', schema = ['role', 'institution', 'institution_id'])\n",
    "not_found_df = not_found_df.drop_nulls() # remove entries for diocese level roles \n",
    "\n",
    "#not_found contains an entry per row where a combination was not found - here we want just one row per unique combination\n",
    "#these combinations could be found much more efficiently, but as it's a byproduct of finding the fg_inst_role_id for all the other rows, this is fine\n",
    "not_found_df = not_found_df.unique()\n",
    "#since the institution names are quite specific, it's not realistic that two roles with the same label but different institution_id could exist\n",
    "\n",
    "#add role details\n",
    "not_found_df = not_found_df.join(\n",
    "    wiag_roles_df.rename({'id' : 'role_id', 'factgrid_id': 'role_fg_id'}), how='left', left_on='role', right_on='name'\n",
    ")\n",
    "#add instution details\n",
    "not_found_df = not_found_df.join(factgrid_institution_df, how='left', left_on='institution_id', right_on='fg_gsn_id')\n",
    "\n",
    "#create label\n",
    "not_found_df = not_found_df.with_columns(Lde = pl.col('role') + ' ' + pl.col('institution'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell generates the translations of the labels. This can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"**Role:** You are a professional translator specializing in historical and religious terminology, with expertise in German–English translation.\n",
    "    **Task:** You will receive a German name for a role or occupation including a place that this role is associated with. Your task is to return the most accurate and context-appropriate English translation.\n",
    "    **Format:** Only return the translation. Do not add any remarks or formatting. Always start the translation with a capital letter.\"\"\"\n",
    "\n",
    "create_miss_inst_roles = scripts.translate.translate(not_found_df, system_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this cell generates the file and show a sample of the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add other columns\n",
    "create_miss_inst_roles = create_miss_inst_roles.with_columns(\n",
    "    qid = None,\n",
    "    Dde = None,\n",
    "    Den = None,\n",
    "    P2 = pl.lit('Q257052'),\n",
    "    P131 = pl.lit('Q153178'),\n",
    "    P3 = pl.col('role_fg_id'),\n",
    "    P267 = pl.col('fg_institution_id'),\n",
    "    # id is the number of the role in the role table in WIAG -- institution_id is the klosterdatenbank id of the institution\n",
    "    P1100 = pl.when(pl.col('role_id').is_null()).then(pl.lit(None)).otherwise('off' + pl.col('role_id').cast(str) + '_gsn' + pl.col('institution_id').cast(str))\n",
    ").select(['qid', 'Lde', 'Len', 'Dde', 'Den', 'P2', 'P131', 'P3', 'P267', 'P1100']) # selecting only relevant columns\n",
    "\n",
    "#export to csv file\n",
    "create_miss_inst_roles.write_csv(os.path.join(output_path, f\"create-missing-inst-roles_{today_string}.csv\"))\n",
    "print(f'{create_miss_inst_roles.height} rows were written. Here is a sample of them:')\n",
    "if create_miss_inst_roles.height >= 3:\n",
    "    display(create_miss_inst_roles.sample(n = 3))\n",
    "else:\n",
    "    display(create_miss_inst_roles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Missing offices\n",
    "\n",
    "\n",
    "\n",
    "### Ignore all missing (inst role) entries now **\n",
    "\n",
    "\n",
    "\n",
    "The code below ignores entries that are generated above and does a join without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_joined_df = joined_df.join(pl.DataFrame(data_dict, schema = ['id', 'fg_inst_role_id'], orient = 'row'), on = 'id')\n",
    "print(len(final_joined_df))\n",
    "final_joined_df.sample(n = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse dates\n",
    "\n",
    "\n",
    "\n",
    "The following code parses the date information present in the date_begin or date_end string and converts it to the correct property in FactGrid and it's corresponding value.\n",
    "\n",
    "There are also testcases which are run in case you want to modify it.\n",
    "\n",
    "Here is an overview of relevant FactGrid properties: [link](https://database.factgrid.de/query/embed.html#SELECT%20%3FPropertyLabel%20%3FProperty%20%3FPropertyDescription%20%3Freciprocal%20%3FreciprocalLabel%20%3Fexample%20%3Fuseful_statements%20%3Fwd%20WHERE%20%7B%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22.%20%7D%0A%20%20%3FProperty%20wdt%3AP8%20wd%3AQ77483.%0A%20%20OPTIONAL%20%7B%20%3FProperty%20wdt%3AP364%20%3Fexample.%20%7D%0A%20%20OPTIONAL%20%7B%20%3FProperty%20wdt%3AP86%20%3Freciprocal.%20%7D%0A%20%20OPTIONAL%20%7B%20%3FProperty%20wdt%3AP343%20%3Fwd.%20%7D%0A%20%20OPTIONAL%20%7B%20%3FProperty%20wdt%3AP310%20%3Fuseful_statements.%20%7D%0A%7D%0AORDER%20BY%20%3FPropertyLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining an enum to more clearly define what type of date is being passed \n",
    "class DateType(Enum):\n",
    "    ONLY_DATE = 0\n",
    "    BEGIN_DATE = 1\n",
    "    END_DATE = 2\n",
    "\n",
    "#date precision and calendar declaration (see Time at https://www.wikidata.org/wiki/Help:QuickStatements#Add_simple_statement)\n",
    "PRECISION_CENTURY = 7\n",
    "PRECISION_DECADE = 8\n",
    "PRECISION_YEAR = 9\n",
    "PRECISION_MONTH = 10\n",
    "PRECISION_DAY = 11\n",
    "JULIAN_ENDING = '/J'\n",
    "\n",
    "#defining some constants for better readability of the code:\n",
    "#self defined:\n",
    "JHS_GROUP = r'(Jhs\\.|Jahrhunderts?)'\n",
    "JH_GROUP = r'(Jh\\.|Jahrhundert)'\n",
    "EIGTH_OF_A_CENTURY = 13\n",
    "QUARTER_OF_A_CENTURY = 25\n",
    "TENTH_OF_A_CENTURY = 10\n",
    "\n",
    "ANTE_GROUP = \"bis|vor|spätestens\"\n",
    "POST_GROUP = \"nach|frühestens|ab|zwischen\" # NOTE: 'zwischen' does not actually fit into this group, but because the current strategy for 'zwischen 1087 und 1093' is to just take the first date with post quem, it makes sense to have it here\n",
    "CIRCA_GROUP = r\"etwa|ca\\.|um\"\n",
    "#pre-compiling the most complex pattern to increase efficiency\n",
    "MOST_COMPLEX_PATTERN = re.compile(r'(wohl )?((kurz )?(' + ANTE_GROUP + '|' + POST_GROUP + r') )?((' + CIRCA_GROUP +r') )?(\\d{3,4})(\\?)?')\n",
    "\n",
    "#FactGrid properties:\n",
    "#simple date properties:\n",
    "DATE = 'P106' \n",
    "BEGIN_DATE = 'P49'\n",
    "END_DATE = 'P50'\n",
    "#when there is uncertainty / when all we know is the latest/earliest possible date:\n",
    "DATE_AFTER = 'P41' # the earliest possible date for something\n",
    "DATE_BEFORE = 'P43' # the latest possible date for something\n",
    "END_TERMINUS_ANTE_QUEM = 'P1123' # latest possible date of the end of a period\n",
    "BEGIN_TERMINUS_ANTE_QUEM  = 'P1124' # latest possible date of the begin of a period\n",
    "END_TERMINUS_POST_QUEM = 'P1125' # earliest possible date of the end of a period\n",
    "BEGIN_TERMINUS_POST_QUEM = 'P1126' # earliest possible date of the beginning of a period\n",
    "\n",
    "NOTE = 'P73' # Field for free notes\n",
    "PRECISION_DATE = 'P467' # FactGrid qualifier for the specific determination of the exactness of a date\n",
    "PRECISION_BEGIN_DATE = 'P785'   # qualifier to specify a begin date\n",
    "PRECISION_END_DATE = 'P786'\n",
    "STRING_PRECISION_BEGIN_DATE = 'P787' # qualifier to specify a begin date; string alternate to P785\n",
    "STRING_PRECISION_END_DATE = 'P788'\n",
    "\n",
    "#qualifiers/options\n",
    "SHORTLY_BEFORE = 'Q255211'\n",
    "SHORTLY_AFTER = 'Q266009'\n",
    "LIKELY = 'Q23356'\n",
    "CIRCA = 'Q10'\n",
    "OR_FOLLOWING_YEAR = 'Q912616'\n",
    "\n",
    "def format_datetime(entry: datetime, precision: int):\n",
    "    ret_val =  f\"+{entry.isoformat()}Z/{precision}\"\n",
    "\n",
    "    if entry.year < 1582: # declaring that the julian calendar is being used by adding '/J' to the end\n",
    "        ret_val +=  JULIAN_ENDING\n",
    "    \n",
    "    #on FactGrid, if the date is at most accurate to a year, the day and month are set to 0. The datetime type in Python does not allow you to set the day or month to 0 so we need to replace it manually\n",
    "    if precision <= PRECISION_YEAR:\n",
    "        ret_val = ret_val.replace(f\"{entry.year}-01-01\", f\"{entry.year}-00-00\", 1)\n",
    "    elif precision == PRECISION_MONTH:\n",
    "        ret_val = ret_val.replace(f\"{entry.year}-{entry.month}-01\", f\"{entry.year}-{entry.month}-00\", 1)\n",
    "\n",
    "    return ret_val\n",
    "\n",
    "#only_date=True means there is only one date, not a 'begin date' and an 'end date'\n",
    "def date_parsing(date_string: str, date_type: DateType):\n",
    "    qualifier = \"\"\n",
    "    precision = PRECISION_CENTURY\n",
    "\n",
    "    ante_property = (match := re.search(ANTE_GROUP, date_string))\n",
    "    post_property = (match := re.search(POST_GROUP, date_string))\n",
    "    assert(not ante_property or not post_property)\n",
    "    \n",
    "    match date_type:\n",
    "        case DateType.ONLY_DATE:\n",
    "            string_precision_qualifier_clause = NOTE\n",
    "            exact_precision_qualifier = PRECISION_DATE\n",
    "            if ante_property:\n",
    "                return_property = DATE_BEFORE\n",
    "            elif post_property:\n",
    "                return_property = DATE_AFTER\n",
    "            else:\n",
    "                return_property = DATE\n",
    "        case DateType.BEGIN_DATE:\n",
    "            string_precision_qualifier_clause = STRING_PRECISION_BEGIN_DATE\n",
    "            exact_precision_qualifier = PRECISION_BEGIN_DATE\n",
    "            if ante_property:\n",
    "                return_property = BEGIN_TERMINUS_ANTE_QUEM\n",
    "            elif post_property:\n",
    "                return_property = BEGIN_TERMINUS_POST_QUEM\n",
    "            else:\n",
    "                return_property = BEGIN_DATE\n",
    "        case DateType.END_DATE:\n",
    "            string_precision_qualifier_clause = STRING_PRECISION_END_DATE\n",
    "            exact_precision_qualifier = PRECISION_END_DATE\n",
    "            if ante_property:\n",
    "                return_property = END_TERMINUS_ANTE_QUEM\n",
    "            elif post_property:\n",
    "                return_property = END_TERMINUS_POST_QUEM\n",
    "            else:\n",
    "                return_property = END_DATE\n",
    "        case _:\n",
    "            assert False, \"Unexpected DateType!\"\n",
    "        \n",
    "    string_precision_qualifier_clause += f'\\t\"{date_string}\"'\n",
    "\n",
    "    if date_string == '?':\n",
    "        return tuple()\n",
    "    \n",
    "    # something like: 12. Jahrhundert\n",
    "    if matches := re.match(r'(\\d{1,2})\\. ' + JH_GROUP, date_string):\n",
    "        year = 100 * int(matches.group(1))\n",
    "    \n",
    "    # something like: 2. Hälfte des 12. Jahrhunderts\n",
    "    elif matches := re.match(r'(\\d)\\. Hälfte (des )?(\\d{1,2})\\. ' + JHS_GROUP, date_string):\n",
    "        half = int(matches.group(1)) - 1\n",
    "        centuries = int(matches.group(3)) - 1\n",
    "        year   = centuries * 100 + (half * 50) + QUARTER_OF_A_CENTURY\n",
    "        qualifier = string_precision_qualifier_clause\n",
    "    \n",
    "    elif matches := re.match(r'(\\w+) Viertel des (\\d{1,2})\\. ' + JHS_GROUP, date_string):\n",
    "        number_map = {\n",
    "            \"erstes\":  0,\n",
    "            \"zweites\": 1,\n",
    "            \"drittes\": 2,\n",
    "            \"viertes\": 3,\n",
    "        }\n",
    "        quarter = matches.group(1)\n",
    "        centuries = int(matches.group(2))\n",
    "        year = (centuries - 1) * 100 + (number_map[quarter] * 25) + EIGTH_OF_A_CENTURY\n",
    "        qualifier = string_precision_qualifier_clause\n",
    "\n",
    "    elif matches := re.match(r'frühes (\\d{1,2})\\. ' + JH_GROUP, date_string):\n",
    "        centuries = int(matches.group(1)) - 1\n",
    "        year = centuries * 100 + TENTH_OF_A_CENTURY\n",
    "        qualifier = string_precision_qualifier_clause\n",
    "\n",
    "    elif matches := re.match(r'spätes (\\d{1,2})\\. ' + JH_GROUP, date_string):\n",
    "        centuries = int(matches.group(1))\n",
    "        year = centuries * 100 - TENTH_OF_A_CENTURY\n",
    "        qualifier = string_precision_qualifier_clause\n",
    "\n",
    "    elif matches := re.match(r'(Anfang|Mitte|Ende) (\\d{1,2})\\. ' + JH_GROUP, date_string):\n",
    "        number_map = {\n",
    "            \"Anfang\":  0,\n",
    "            \"Mitte\": 1,\n",
    "            \"Ende\": 2,\n",
    "        }\n",
    "        third = number_map[matches.group(1)]\n",
    "        centuries = int(matches.group(2)) - 1\n",
    "        year = centuries * 100 + (third * 33) + 17\n",
    "        qualifier = string_precision_qualifier_clause\n",
    "\n",
    "    elif matches := re.match(r'(\\d{3,4})er Jahre', date_string):\n",
    "        year = int(matches.group(1))\n",
    "        precision = PRECISION_DECADE\n",
    "    \n",
    "    elif matches := re.match(r'Wende zum (\\d{1,2})\\. ' + JH_GROUP, date_string):\n",
    "        centuries = int(matches.group(1)) - 1\n",
    "        year = centuries * 100 - 10\n",
    "        qualifier = string_precision_qualifier_clause\n",
    "\n",
    "    elif matches := re.match(r'Anfang der (\\d{3,4})er Jahre', date_string):\n",
    "        year = int(matches.group(1))\n",
    "        qualifier = string_precision_qualifier_clause\n",
    "        precision = PRECISION_DECADE\n",
    "\n",
    "    # something like: (1140) 1145\n",
    "    elif matches := re.match(r'\\((\\d{3,4})\\s?\\?\\) (\\d{3,4})', date_string):\n",
    "        year = int(matches.group(2)) # ignoring the year in parantheses\n",
    "        precision = PRECISION_YEAR\n",
    "        qualifier = string_precision_qualifier_clause\n",
    "    \n",
    "    # something like: zwischen 1087 und 1093\n",
    "    elif matches := re.match(r'zwischen (\\d{3,4}) und (\\d{3,4})', date_string):\n",
    "        year = int(matches.group(1)) # ignoring the second year\n",
    "        precision = PRECISION_YEAR\n",
    "        qualifier = string_precision_qualifier_clause\n",
    "\n",
    "    # something like: 1140/1141\n",
    "    # or like: 1140/1152\n",
    "    elif matches := re.match(r'(\\d{3,4})/(\\d{3,4})', date_string):\n",
    "        year1 = int(matches.group(1))\n",
    "        year2 = int(matches.group(2))\n",
    "\n",
    "        if year2 - year1 == 1:\n",
    "            # check for consecutive years\n",
    "            qualifier = exact_precision_qualifier + '\\t' + OR_FOLLOWING_YEAR\n",
    "        else:\n",
    "            qualifier = string_precision_qualifier_clause\n",
    "            \n",
    "        year = year1\n",
    "        precision = PRECISION_YEAR\n",
    "\n",
    "    # this pattern is pre-compiled above, because it's rather complex and it's much more efficient to compile it just once, instead of on every function call\n",
    "    elif matches := MOST_COMPLEX_PATTERN.match(date_string):\n",
    "        if matches.group(1): # if 'wohl' was found\n",
    "            qualifier = exact_precision_qualifier + '\\t' + LIKELY\n",
    "        if matches.group(5): # if 'etwa' , 'ca.' or 'um' were found\n",
    "            if len(qualifier) != 0:\n",
    "                qualifier += '\\t'\n",
    "            qualifier += exact_precision_qualifier + '\\t' + CIRCA\n",
    "                \n",
    "        if matches.group(3): # if 'kurz' was found -- because of how the regex is defined, this can only happen when combined with 'nach', 'bis', etc.\n",
    "            if len(qualifier) != 0:\n",
    "                qualifier += '\\t'\n",
    "\n",
    "            if ante_property: # already checked above whether it's before or after\n",
    "                qualifier += exact_precision_qualifier + '\\t' + SHORTLY_BEFORE\n",
    "            else: # post_property\n",
    "                qualifier += exact_precision_qualifier + '\\t' + SHORTLY_AFTER\n",
    "\n",
    "        if matches.group(8): # if a question mark at the end were found\n",
    "            # TODO is it correct, that on ? the other matches ('ca.' etc.) are ignored, because it's not exact enough?\n",
    "            qualifier = string_precision_qualifier_clause\n",
    "        \n",
    "        year = int(matches.group(7))\n",
    "        precision = PRECISION_YEAR\n",
    "\n",
    "    else:\n",
    "        raise Exception(f\"Couldn't parse date '{date_string}'\")\n",
    "\n",
    "    entry = datetime(year, 1, 1)\n",
    "    return (return_property, format_datetime(entry, precision), qualifier, date(year, 1, 1).isoformat())\n",
    "    #return (return_property, format_datetime(entry, precision), qualifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test cases\n",
    "\n",
    "\n",
    "\n",
    "Because there are so many special cases, testing is a must to more clearly show what is expected for each case and make sure no incorrect changes are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO still to be handled:\n",
    "    # nach 1177/vor 1305 -- maybe correct to \"zwischen 1177 und 1305\"?\n",
    "    # \"(996)\" -- mistake or what does this mean?\n",
    "    # \"12. oder 13. Jahrhundert\"\n",
    "    # \"Ende 11. Jahrhundert/1. Viertel 12. Jahrhundert\"\n",
    "    # \"(vor 1254) 1256\"\n",
    "\n",
    "begin_date_tests = {\n",
    "    \"1605\": (BEGIN_DATE, \"+1605-00-00T00:00:00Z/9\"),\n",
    "    \"1205\": (BEGIN_DATE, \"+1205-00-00T00:00:00Z/9/J\"),\n",
    "    \"1205?\": (BEGIN_DATE, \"+1205-00-00T00:00:00Z/9/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"1205?\"'),\n",
    "    \"12. Jahrhundert\": (BEGIN_DATE, \"+1200-00-00T00:00:00Z/7/J\"),\n",
    "    \"1. Hälfte des 12. Jhs.\": (BEGIN_DATE, \"+1125-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"1. Hälfte des 12. Jhs.\"'),\n",
    "    \"1. Hälfte des 12. Jahrhunderts\": (BEGIN_DATE, \"+1125-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"1. Hälfte des 12. Jahrhunderts\"'),\n",
    "    \"2. Hälfte des 12. Jhs.\": (BEGIN_DATE, \"+1175-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"2. Hälfte des 12. Jhs.\"'),\n",
    "    \"erstes Viertel des 12. Jhs.\": (BEGIN_DATE, \"+1113-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"erstes Viertel des 12. Jhs.\"'),\n",
    "    \"zweites Viertel des 12. Jhs.\": (BEGIN_DATE, \"+1138-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"zweites Viertel des 12. Jhs.\"'),\n",
    "    \"drittes Viertel des 12. Jhs.\": (BEGIN_DATE, \"+1163-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"drittes Viertel des 12. Jhs.\"'),\n",
    "    \"viertes Viertel des 12. Jhs.\": (BEGIN_DATE, \"+1188-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"viertes Viertel des 12. Jhs.\"'),\n",
    "    \"frühes 12. Jh.\": (BEGIN_DATE, \"+1110-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"frühes 12. Jh.\"'),\n",
    "    \"spätes 12. Jh.\": (BEGIN_DATE, \"+1190-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"spätes 12. Jh.\"'),\n",
    "    \"Anfang 12. Jh.\": (BEGIN_DATE, \"+1117-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"Anfang 12. Jh.\"'),\n",
    "    \"Anfang 15. Jahrhundert\": (BEGIN_DATE, \"+1417-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"Anfang 15. Jahrhundert\"'),\n",
    "    \"Mitte 12. Jh.\": (BEGIN_DATE, \"+1150-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"Mitte 12. Jh.\"'),\n",
    "    \"Mitte 14. Jahrhundert?\": (BEGIN_DATE, \"+1350-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"Mitte 14. Jahrhundert?\"'),\n",
    "    \"Ende 12. Jh.\": (BEGIN_DATE, \"+1183-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"Ende 12. Jh.\"'),\n",
    "    \"Ende 12. Jahrhundert\": (BEGIN_DATE, \"+1183-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"Ende 12. Jahrhundert\"'),\n",
    "    \"bis etwa 1147\": (BEGIN_TERMINUS_ANTE_QUEM, '+1147-00-00T00:00:00Z/9/J', PRECISION_BEGIN_DATE + '\\t' + CIRCA),\n",
    "    \"etwa 1147\": (BEGIN_DATE, '+1147-00-00T00:00:00Z/9/J', PRECISION_BEGIN_DATE + '\\t' + CIRCA),\n",
    "    \"ca. 1050\": (BEGIN_DATE, \"+1050-00-00T00:00:00Z/9/J\", PRECISION_BEGIN_DATE + '\\t' + CIRCA),\n",
    "    \"um 1050\": (BEGIN_DATE, \"+1050-00-00T00:00:00Z/9/J\", PRECISION_BEGIN_DATE + '\\t' + CIRCA),\n",
    "    \"1230er Jahre\": (BEGIN_DATE, \"+1230-00-00T00:00:00Z/8/J\"),\n",
    "    \"Wende zum 12. Jh.\": (BEGIN_DATE, '+1090-00-00T00:00:00Z/7/J', STRING_PRECISION_BEGIN_DATE + '\\t\"Wende zum 12. Jh.\"'),\n",
    "    \"Anfang der 1480er Jahre\": (BEGIN_DATE, '+1480-00-00T00:00:00Z/8/J', STRING_PRECISION_BEGIN_DATE + '\\t\"Anfang der 1480er Jahre\"'),\n",
    "    \"1164/1165\": (BEGIN_DATE, '+1164-00-00T00:00:00Z/9/J', PRECISION_BEGIN_DATE + '\\t' + OR_FOLLOWING_YEAR),\n",
    "    \"1164/1177\": (BEGIN_DATE, '+1164-00-00T00:00:00Z/9/J', STRING_PRECISION_BEGIN_DATE + '\\t\"1164/1177\"'),\n",
    "    \"(1014?) 1015\": (BEGIN_DATE,\"+1015-00-00T00:00:00Z/9/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"(1014?) 1015\"'),\n",
    "    \"ab 1534\": (BEGIN_TERMINUS_POST_QUEM, '+1534-00-00T00:00:00Z/9/J'),\n",
    "    \"nach 1230\": (BEGIN_TERMINUS_POST_QUEM, '+1230-00-00T00:00:00Z/9/J'),\n",
    "    \"kurz nach 1200\": (BEGIN_TERMINUS_POST_QUEM, '+1200-00-00T00:00:00Z/9/J', PRECISION_BEGIN_DATE + '\\t' + SHORTLY_AFTER),\n",
    "    \"frühestens 1342\": (BEGIN_TERMINUS_POST_QUEM, '+1342-00-00T00:00:00Z/9/J'),\n",
    "    \"vor 1230\": (BEGIN_TERMINUS_ANTE_QUEM, '+1230-00-00T00:00:00Z/9/J'),\n",
    "    \"wohl vor 1249\": (BEGIN_TERMINUS_ANTE_QUEM, '+1249-00-00T00:00:00Z/9/J', PRECISION_BEGIN_DATE + '\\t' + LIKELY),\n",
    "    \"kurz vor 1200\": (BEGIN_TERMINUS_ANTE_QUEM, '+1200-00-00T00:00:00Z/9/J', PRECISION_BEGIN_DATE + '\\t' + SHORTLY_BEFORE), \n",
    "    \"wohl etwa 1249\": (BEGIN_DATE, '+1249-00-00T00:00:00Z/9/J', PRECISION_BEGIN_DATE + '\\t' + LIKELY + '\\t' + PRECISION_BEGIN_DATE + '\\t' + CIRCA),\n",
    "    \"spätestens 1277\": (BEGIN_TERMINUS_ANTE_QUEM, '+1277-00-00T00:00:00Z/9/J'),\n",
    "    \"zwischen 1087 und 1093\": (BEGIN_TERMINUS_POST_QUEM,\"+1087-00-00T00:00:00Z/9/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"zwischen 1087 und 1093\"'),\n",
    "}\n",
    "\n",
    "for key, value in begin_date_tests.items():\n",
    "    retval = date_parsing(key, DateType.BEGIN_DATE)\n",
    "    if len(retval[2]) == 0:\n",
    "        retval = retval[0:2]\n",
    "    else:\n",
    "        retval = retval[0:3] # ignore the datetime object\n",
    "    assert retval == value, f\"{key}: Returned {retval} instead of {value}\"\n",
    "\n",
    "end_date_tests = {\n",
    "    \"1205?\": (END_DATE, \"+1205-00-00T00:00:00Z/9/J\", STRING_PRECISION_END_DATE + '\\t\"1205?\"'),\n",
    "    \"12. Jahrhundert\": (END_DATE, \"+1200-00-00T00:00:00Z/7/J\"),\n",
    "    \"drittes Viertel des 12. Jhs.\": (END_DATE, \"+1163-00-00T00:00:00Z/7/J\", STRING_PRECISION_END_DATE + '\\t\"drittes Viertel des 12. Jhs.\"'),\n",
    "    \"bis etwa 1147\": (END_TERMINUS_ANTE_QUEM, '+1147-00-00T00:00:00Z/9/J', PRECISION_END_DATE + '\\t' + CIRCA),\n",
    "    \"um 1050\": (END_DATE, \"+1050-00-00T00:00:00Z/9/J\", PRECISION_END_DATE + '\\t' + CIRCA),\n",
    "    \"Anfang der 1480er Jahre\": (END_DATE, '+1480-00-00T00:00:00Z/8/J', STRING_PRECISION_END_DATE + '\\t\"Anfang der 1480er Jahre\"'),\n",
    "    \"1164/1165\": (END_DATE, '+1164-00-00T00:00:00Z/9/J', PRECISION_END_DATE + '\\t' + OR_FOLLOWING_YEAR),\n",
    "    \"1164/1177\": (END_DATE, '+1164-00-00T00:00:00Z/9/J', STRING_PRECISION_END_DATE + '\\t\"1164/1177\"'),\n",
    "    \"(1014?) 1015\": (END_DATE,\"+1015-00-00T00:00:00Z/9/J\", STRING_PRECISION_END_DATE + '\\t\"(1014?) 1015\"'),\n",
    "    \"ab 1534\": (END_TERMINUS_POST_QUEM, '+1534-00-00T00:00:00Z/9/J'),\n",
    "    \"nach 1230\": (END_TERMINUS_POST_QUEM, '+1230-00-00T00:00:00Z/9/J'),\n",
    "    \"frühestens 1342\": (END_TERMINUS_POST_QUEM, '+1342-00-00T00:00:00Z/9/J'),\n",
    "    \"vor 1230\": (END_TERMINUS_ANTE_QUEM, '+1230-00-00T00:00:00Z/9/J'),\n",
    "    \"wohl vor 1249\": (END_TERMINUS_ANTE_QUEM, '+1249-00-00T00:00:00Z/9/J', PRECISION_END_DATE + '\\t' + LIKELY),\n",
    "    \"zwischen 1087 und 1093\": (END_TERMINUS_POST_QUEM,\"+1087-00-00T00:00:00Z/9/J\", STRING_PRECISION_END_DATE + '\\t\"zwischen 1087 und 1093\"'),\n",
    "}\n",
    "\n",
    "for key, value in end_date_tests.items():\n",
    "    retval = date_parsing(key, DateType.END_DATE)\n",
    "    if len(retval[2]) == 0:\n",
    "        retval = retval[0:2]\n",
    "    else:\n",
    "        retval = retval[0:3] # ignore the datetime object\n",
    "    assert retval == value, f\"{key}: Returned {retval} instead of {value}\"\n",
    "\n",
    "only_date_tests = {\n",
    "    \"1205?\": (DATE, \"+1205-00-00T00:00:00Z/9/J\", NOTE + '\\t\"1205?\"'),\n",
    "    \"12. Jahrhundert\": (DATE, \"+1200-00-00T00:00:00Z/7/J\"),\n",
    "    \"drittes Viertel des 12. Jhs.\": (DATE, \"+1163-00-00T00:00:00Z/7/J\", NOTE + '\\t\"drittes Viertel des 12. Jhs.\"'),\n",
    "    \"bis etwa 1147\": (DATE_BEFORE, '+1147-00-00T00:00:00Z/9/J', PRECISION_DATE + '\\t' + CIRCA),\n",
    "    \"um 1050\": (DATE, \"+1050-00-00T00:00:00Z/9/J\", PRECISION_DATE + '\\t' + CIRCA),\n",
    "    \"Anfang der 1480er Jahre\": (DATE, '+1480-00-00T00:00:00Z/8/J', NOTE + '\\t\"Anfang der 1480er Jahre\"'),\n",
    "    \"1164/1165\": (DATE, '+1164-00-00T00:00:00Z/9/J', PRECISION_DATE + '\\t' + OR_FOLLOWING_YEAR),\n",
    "    \"1164/1177\": (DATE, '+1164-00-00T00:00:00Z/9/J', NOTE + '\\t\"1164/1177\"'),\n",
    "    \"(1014?) 1015\": (DATE,\"+1015-00-00T00:00:00Z/9/J\", NOTE + '\\t\"(1014?) 1015\"'),\n",
    "    \"ab 1534\": (DATE_AFTER, '+1534-00-00T00:00:00Z/9/J'),\n",
    "    \"nach 1230\": (DATE_AFTER, '+1230-00-00T00:00:00Z/9/J'),\n",
    "    \"frühestens 1342\": (DATE_AFTER, '+1342-00-00T00:00:00Z/9/J'),\n",
    "    \"vor 1230\": (DATE_BEFORE, '+1230-00-00T00:00:00Z/9/J'),\n",
    "    \"wohl vor 1249\": (DATE_BEFORE, '+1249-00-00T00:00:00Z/9/J', PRECISION_DATE + '\\t' + LIKELY),\n",
    "    \"zwischen 1087 und 1093\": (DATE_AFTER,\"+1087-00-00T00:00:00Z/9/J\", NOTE + '\\t\"zwischen 1087 und 1093\"'),\n",
    "}\n",
    "\n",
    "for key, value in only_date_tests.items():\n",
    "    retval = date_parsing(key, DateType.ONLY_DATE)\n",
    "    if len(retval[2]) == 0:\n",
    "        retval = retval[0:2]\n",
    "    else:\n",
    "        retval = retval[0:3] # ignore the datetime object\n",
    "    assert retval == value, f\"{key}: Returned {retval} instead of {value}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How a date is parsed depends on whether it's the only date or not (begin and end date), so the below function handles this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_both_dates(d: dict):\n",
    "    try:\n",
    "        date_begin = d[\"date_begin\"]\n",
    "        date_end = d[\"date_end\"]\n",
    "\n",
    "        if date_begin != None:\n",
    "            if date_end != None:\n",
    "                begin = date_parsing(date_begin, DateType.BEGIN_DATE)\n",
    "                end = date_parsing(date_end, DateType.END_DATE)\n",
    "            \n",
    "                date_clauses = {\"begin\": begin, \"end\" : end}\n",
    "            else:\n",
    "                date_clauses = {\"begin\": date_parsing(date_begin, DateType.ONLY_DATE), \"end\" : None}\n",
    "        else:\n",
    "            if date_end != None:\n",
    "                date_clauses = {\"begin\": None, \"end\" : date_parsing(date_end, DateType.ONLY_DATE)}\n",
    "            else:\n",
    "                # do nothing, since nothing needs to be parsed\n",
    "                date_clauses = {\"begin\": None, \"end\" : None}\n",
    "\n",
    "        return date_clauses\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        print(row)\n",
    "        print('\\n')\n",
    "        return {\"begin\": None, \"end\" : None}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate missing offices file\n",
    "\n",
    "\n",
    "\n",
    "The code below creates the office entries to be uploaded on factgrid.\n",
    "\n",
    "\n",
    "\n",
    "If the date parsing function can't handle a date (either because that format hasn't been encountered yet or because the entry is nonsense), it prints the problematic date and the corresponding entry from the dataframe. If the relevant rows contain some nonsense data, use this output to find and fix it. If the data is not nonsense, most likely the date_parsing function above needs to be extended. For this, you probably want to contact whoever is responsible for maintaining the sync_notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(output_path, f'quickstatements-offices_{today_string}.qs')\n",
    "\n",
    "with open(filepath, 'w') as file:\n",
    "    for row in final_joined_df.iter_rows(named = True):\n",
    "        try:\n",
    "            date_clauses = ()\n",
    "\n",
    "            if row['date_begin'] != None:\n",
    "                if row['date_end'] != None:\n",
    "                    date_clauses = (*date_parsing(row['date_begin'], DateType.BEGIN_DATE), *date_parsing(row['date_end'], DateType.END_DATE))\n",
    "                else:\n",
    "                    date_clauses = date_parsing(row['date_begin'], DateType.ONLY_DATE)\n",
    "            else:\n",
    "                if row['date_end'] != None:\n",
    "                    date_clauses = date_parsing(row['date_end'], DateType.ONLY_DATE)\n",
    "                    \n",
    "            file.write('\\t'.join([\n",
    "                row['FactGrid'], \n",
    "                'P165', \n",
    "                row['fg_inst_role_id'],\n",
    "                'S601', \n",
    "                '\"' + row['person_id'] + '\"',\n",
    "                *date_clauses,\n",
    "            ]) + '\\n')\n",
    "        except Exception as e:\n",
    "            print(traceback.format_exc())\n",
    "            print(row)\n",
    "            print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Updating FactGrid\n",
    "\n",
    "Once the files have been generated, please open [QuickStatements](https://database.factgrid.de/quickstatements/#/batch) and **run the CSV-commands/V1-commands** (the qs-file contains V1 commands, the csv-files CSV-commands). More details to perform this can be found [here](https://github.com/WIAG-ADW-GOE/sync_notebooks/blob/main/docs/Run_factgrid_csv.md).\n",
    "\n",
    "### Next notebook\n",
    "\n",
    "Once the update is done, you can continue with [notebook 5](fg_to_dpr.ipynb) (fg_to_dpr)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
